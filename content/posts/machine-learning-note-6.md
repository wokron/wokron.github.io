+++
title = "机器学习笔记之正则化"
tags = ["数学"]
categories = ["人工智能"]
series = ["机器学习笔记"]
aliases = ["/posts/6d380668"]
date = "2022-10-08T19:27:57+08:00"
+++
## 一、前言——过拟合
我们知道，经过一组点可以有无数条曲线。这些曲线对于这组样本点的损失函数同为 0。但是对于预测来说，这些曲线产生的结果却并不相同。这就意味着，进行梯度下降到达某一最低点时，依旧不一定能得到“最好的”预测（拟合/分类）效果。甚至可能对于一些情况，此时的（预测/分类）效果反而更差了。这样的情况就称为过拟合。

过拟合的存在是很合理的。从感性上讲，将机器学习的过程类比人类的认知，一个观念的形成不能超出经验之外，认知的结果永远是片面而非客观的。那么在与更广泛的客观现实接触之前，我们必然无法得知已经形成的认知是否是依旧可以应用的。

这是一个很休谟的观点。但却无法解决现实问题。我们依旧需要找到减少过拟合的方法。

## 二、惩罚
我们的思想中存在着一种先验观念，它规范天地万物，在冥冥中告诉我们什么是“合理的”。对于机器学习模型来说也是一样的，它应当具有这样的机制，告诉它什么情况是不可能的。

就比如说，对于房价，我们知道一些特征是更加重要的，而另一些是更加不重要的。很显然，那些更重要的特征对应的权重应该较不重要的特征对应的权重大。那么我们就需要对那些不重要的权重进行“惩罚”，以避免这些权重过大，从而导致模型过拟合。这样的“惩罚”在损失函数中体现。即，当这些权重过大时，损失函数也会相应增大。

具体而言，对如下的表达式
$$
    y = w_1x_1 + w_2x_2 + w_3x_3 + b
$$
假设要使第二、三个权重相对较小，则可以在损失函数中加上惩罚项 $\lambda_2 w_2 + \lambda_3 w_3$。其中 $\lambda_2, \lambda_3$ 取较大值。则损失函数变为
$$
    J_{new}(\vec{w}, b) = J(\vec{w}, b) + \lambda_2 w_2 + \lambda_3 w_3
$$
具体地比如说
$$
    J_{new}(\vec{w}, b) = J(\vec{w}, b) + 1000 w_2 + 2000 w_3
$$
那么此时很显然，当 $w_2, w_3$ 较大时，损失函数也会相应更大。

可是对于大多数情况，我们无法事先知晓权重的重要程度。对于这些一般化的问题，还需要有一般化的解决办法。

## 三、正则化
正则化是惩罚的一种。该方法在损失函数中增加了正则项：
$$
    \frac{\lambda}{2m} \sum_{j=1}^n w^2_j
$$
其中 $\lambda$ 称为正则化参数。

则新的损失函数为
$$
    J_{new}(\vec{w}, b) = J(\vec{w}, b) + \frac{\lambda}{2m} \sum_{j=1}^n w^2_j
$$

这里有几个值得解释的地方：

### 惩罚所有权重
第一是**对所有权重的惩罚**。如果按照上一节的解释，很容易认为，正则化等同于对所有权重的相同比例的惩罚。这是否相当于对所有权重都不惩罚呢？若是如此，那么正则化是否还有意义？

关键在于正则项的 $w^2_j$ 上。这是权重的平方而非权重本身。我们对损失函数求 $w_j$ 的偏导。
$$
    \frac{\partial{J_{new}(\vec{w}, b)}}{\partial{w_j}}
    = \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}}
    + \frac{\lambda}{m} w_j
$$
我们可以得到两个结论。
首先，正则项永远使权重不断趋于0。即便在到达或接近原损失函数最小值时，$\frac{\partial{J(\vec{w}, b)}}{\partial{w_j}} \approx 0$，但此时正则项依旧会发挥作用。这就使得权重无法安定在拟合效果最好的位置。

并且，权重（绝对值）更大的特征，其权重值趋近于 0 的“速度”要快于权重更小的特征。这就避免了对所有权重同一的惩罚。

由此我们也可以看出正则项“眼中”最好的权重是什么样子的。那就是所有权重均为 0。当然，通过合理选择学习率 $\alpha$ 和正则化参数 $\lambda$，是不可能让这种情况发生的。但正则项却可以实实在在地提供一种“拉力”，用来纠正过于“复杂”的模型结构，使得拟合得到的超平面倾向于平缓。


### 除 m 缩放
第二需要解释的是**正则项中的除 m 缩放**。这样做的目的是保证同一正则化参数在不同的样本数量下都有效。

我们以正则化了的多元线性回归损失函数对 $w_j$ 偏导为例。
$$
    \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}}
    = \frac{1}{m}\sum_{i=1}^{m}(\vec{w}\cdot\vec{x}^{(i)} + b - y^{(i)})x^{(i)}_j
    + \frac{\lambda}{m} w_j
$$

假设正则项并不除以 m，或者按照感觉，除以 n。那么在学习率和正则化参数不变的情况下，随着样本量的增加，梯度下降时正则化对权重的影响保持不变；可原损失函数却因为除以了m，对权重的影响减小。这样就会影响正则化参数在不同样本量时的有效性。

因此对正则项也采取除 m。这样在样本量增加时，原损失函数和正则项对权重的影响就是同比例减小的了。

## 四、代码实现
这一部分很简单，就不具体写出来了。