<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wokron.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":"default","style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="StringCat的个人博客。记录学习、分享经验">
<meta property="og:type" content="website">
<meta property="og:title" content="StringCat的博客">
<meta property="og:url" content="https://wokron.github.io/page/5/index.html">
<meta property="og:site_name" content="StringCat的博客">
<meta property="og:description" content="StringCat的个人博客。记录学习、分享经验">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="wokron">
<meta property="article:tag" content="github,java,c#,python,pytorch,blog,engine,buaa,algorithm,software engineering">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wokron.github.io/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>StringCat的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">StringCat的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">学生党踩坑记录</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/55036469/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/55036469/" class="post-title-link" itemprop="url">机器学习之决策树</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-17 08:07:31" itemprop="dateCreated datePublished" datetime="2022-10-17T08:07:31+08:00">2022-10-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>决策树是另一种十分有效的机器学习模型。该模型采取树状分支结构，能够很快地进行模型训练，并具有较高的准确率。</p>
<p>本文将以决策树的最简单形式开始，即特征只包括真值特征，或只有两种选择的特征，且结果也只有两种的情况。</p>
<h2 id="二、决策树基本模型"><a href="#二、决策树基本模型" class="headerlink" title="二、决策树基本模型"></a>二、决策树基本模型</h2><p>决策树是一个二叉树，树的非叶节点存储需要区分的特征，叶节点存储预测的分类。对于每一个要进行预测的数据，从根节点开始，根据当前节点所对应的特征，选择移动到左子节点或右子节点。直到最终移动到一个叶节点，返回预测结果。</p>
<p>可以认为决策树的预测过程就是通过不同特征将当前数据分组归类的过程。决策树的构建过程，或者更一般地说，该模型的学习过程，也就是不断确定如何分类的过程。</p>
<h2 id="三、决策树的构建"><a href="#三、决策树的构建" class="headerlink" title="三、决策树的构建"></a>三、决策树的构建</h2><p>总体上讲，对于我们已有的样本数据，构建决策树的过程时这样的。</p>
<ol>
<li>选择一个特征，将该特征作为当前节点的特征</li>
<li>根据该特征将样本数据分为两组</li>
<li>对于两组被分类的数据，分别递归执行如上操作，从而确定当前节点的左右子节点。直到当前数据已经全部是要预测的某个结果了、或者所有特征都被用完、又或者递归进行到了最大层数。</li>
</ol>
<p>可以看到，决策树构建的过程中大部分过程都可以用简单的程序逻辑实现。现在唯一要解决的就是在每个节点时，如何选择一个特征了。</p>
<blockquote>
<p>当然，对于神经网络来说也是这样的。唯一要解决的是如何选择权重。</p>
</blockquote>
<h2 id="四、特征的选择"><a href="#四、特征的选择" class="headerlink" title="四、特征的选择"></a>四、特征的选择</h2><p>我们要衡量选择不同特征对训练样本的区分效果。这点类似于损失函数。</p>
<p>我们区分的目标是使得所有当前组的样本属于同一个预测结果。那么我们就要找到一种衡量纯度的指标，数据越纯指标越小，数据越不纯指标越大。这种指标就是熵。假设某一种类在数据总体中的占比为 $p_1$，则熵 $H(p_1)$ 为</p>
<script type="math/tex; mode=display">
    p_0 = 1 - p_1 \\
    H(p_1) = -p_1log_2(p_1) - p_0 log_2(p_0)</script><p>即</p>
<script type="math/tex; mode=display">
    H(p_1) = -p_1log_2(p_1) - (1-p_1) log_2(1 - p_1)</script><p>当 $p_1 = 0$ 或 $p_1 = 1$ 时</p>
<script type="math/tex; mode=display">
    H(0) = H(0+0) = 0 \\
    H(1) = H(1-0) = 0</script><blockquote>
<p>可以看出熵与交叉熵损失函数之间有联系。确实，交叉熵损失函数也是用来衡量预测的纯度的。对于一个预测，我们总希望它接近 0 或者 1，也就是纯度最大的比例。</p>
</blockquote>
<p>我们不妨设决策树中预测可能结果中一个为正例，一个为负例。假设对于当前节点处的训练样本集合 $X$， 我们选择第 j 个特征，将该集合分为两个部分 $A<em>j$ 和 $B_j$。设此时 $A_j$ 和 $B_j$ 中正例所占比例分别为 $p</em>{Aj}, p_{Bj}$，则对应的熵为：</p>
<script type="math/tex; mode=display">
    H(p_{Aj}) 和 H(p_{Bj})</script><p>对于不同的 j，两个熵值均会不同，那么我们如何比较哪种选择更为合适呢？我们需要对熵取加权平均，权重为该集合中元素数量占原集合数量的比重。</p>
<script type="math/tex; mode=display">
    \frac{\#A_jH(p_{Aj}) + \#B_jH(p_{Bj})}{\#X}</script><blockquote>
<p>这是很合理的。举个例子，假设选择了一个特征将一个样本和其他样本分开，虽然第一个样本的熵为 0，可是该分类对样本整体的影响较小，决定整体纯度的还是其他样本的部分，最终平均后的熵可能并不会减少太多</p>
</blockquote>
<p>我们将分类后的平均熵值对分类前的总熵的减少量称为信息增益（Information Gain）。假设选择某一特征时，子集合元素数量占原集合的比重为 $w^{left}, w^{right}$，则</p>
<script type="math/tex; mode=display">
    \text{Information Gain} = H(p^{root}_1) - (w^{left}H(p^{left}_1) + w^{right}H(p^{right}_1))</script><p>这样我们就解决了选择特征的问题，只要选择使信息增益最大的特征即可。</p>
<blockquote>
<p>很明显，如果一个特征在祖先节点已经被选择过了，则信息增益为 0。当所有特征的信息增益都为 0 时，说明所有特征都已经被使用过了，此时就可以返回递归了。</p>
</blockquote>
<h2 id="五、多取值特征"><a href="#五、多取值特征" class="headerlink" title="五、多取值特征"></a>五、多取值特征</h2><p>之前，我们的每一个特征都只能有两个取值，我们要将特征的取值推广到多个。这里采用的方法是将多取值特征转化为双取值特征。</p>
<p>解决方案是采用独热（one-hot）编码。举例来说，如果有一个特征 x，取值为1、2或3。那么我们可以将其转化为三个特征 is_x_equal1、is_x_equal2和is_x_equal3。这样，三个新的特征就可以只取 0、1 两个值了。</p>
<h2 id="六、连续取值特征"><a href="#六、连续取值特征" class="headerlink" title="六、连续取值特征"></a>六、连续取值特征</h2><p>对于连续取值的特征，我们要将其转化为离散特征。</p>
<p>具体来说，我们可以取一个阈值 n，将样本按该特征的值分为大于 n 和小于等于 n 的两个部分。为了方便选择，我们可以取样本的值进行分割，也就是分别取每一个样本中该特征对应的值作为划分时所用的值，计算其信息增益，最后选择使信息增益最大的值作为对该特征的划分即可。</p>
<h2 id="七、回归树——对连续取值结果的预测"><a href="#七、回归树——对连续取值结果的预测" class="headerlink" title="七、回归树——对连续取值结果的预测"></a>七、回归树——对连续取值结果的预测</h2><p>回归树是对决策树的进一步扩展。使得决策树模型可以学习结果为连续值的训练样本。</p>
<p>这样，原本的计算信息增益的方法就不再适用了。现在的数据不再有纯度的分别了，取而代之的是疏密的区别。这里我们采用样本结果的方差来衡量数据的离散程度。依旧设子集合元素数量占原集合的比重为 $w^{left}, w^{right}$，样本结果的方差则为 $var^{root}, var^{left}, var^{right}$</p>
<p>则增益的大小为</p>
<script type="math/tex; mode=display">
    \text{Gain} = var^{root} - (w^{left}var^{left} + w^{right}var^{right})</script><p>经过不断的递归构建，我们最终在叶节点处得到一组方差较小的样本。将这组样本的结果取平均，即得到叶节点对应的预测值。</p>
<h2 id="八、决策树林"><a href="#八、决策树林" class="headerlink" title="八、决策树林"></a>八、决策树林</h2><p>决策树的缺点之一是其可能对数据中的微小变化十分敏感，以至于稍有不同的数据就会产生截然不同的决策树结构。对于需要有比较大可靠性的预测，这是需要避免的。</p>
<p>解决方法是生成许多不同结构的决策树，通过这些决策树的预测结果表决或取平均，得到最终的预测结果。但确定的训练样本只会产生确定的决策树。因此这里采用随机化保证生成不同结构的决策树。</p>
<p>也就是说，对于大小为 m 的训练集，我们从中随机放回抽取 m 个样本，组成一个新的训练集，用该训练集生成一个决策树。重复这样的过程多次，就可以得到许多不同结构的决策树。</p>
<blockquote>
<p>还有一种名为 XGBoost 的方法。这样方法在选取样本组成新的训练集的过程中，会倾向于选择之前并未被选取的样本，这样组成的样本集合将更为一般化，从而使得学习效果更好。</p>
</blockquote>
<p>这样，我们就得到了能解决大部分机器学习问题的决策树算法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/5b526508/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/5b526508/" class="post-title-link" itemprop="url">机器学习之模型评估与优化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-16 16:22:46" itemprop="dateCreated datePublished" datetime="2022-10-16T16:22:46+08:00">2022-10-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>对于一个训练好的模型，我们需要知道它是否能很好地适应现实应用。也就是说，我们希望得知该模型对于训练样本之外的数据，是否也能有较好的预测效果。</p>
<blockquote>
<p>模型的这种能力称为泛化能力</p>
</blockquote>
<p>这一问题本质上还是我们之前讨论过的拟合问题。只不过对于一个较为复杂的模型来说，常常不能将其拟合或分类的结果用图像直观表示。如果我们不能较为清晰地评估模型，那就无法进一步进行优化调整（如调整正则化系数、增加或减少特征、增加样本数量等）。因此我们需要方法，来一般化地评估模型的质量。</p>
<h2 id="二、泛化能力的衡量指标"><a href="#二、泛化能力的衡量指标" class="headerlink" title="二、泛化能力的衡量指标"></a>二、泛化能力的衡量指标</h2><p>正如前一节所说，泛化能力的衡量就是衡量模型对训练样本集之外的数据集的预测效果。我们知道衡量模型对训练样本预测效果的指标是损失函数，那么对于其他数据，我们同样可以把损失函数的概念迁移到这里来。</p>
<p>我们可以将已经获得的数据划分为两部分，一部分用于训练模型，称为训练集；另一部分用于测试模型的泛化能力，称为测试集。对于用训练集训练后的数据，我们再求该模型对测试集数据的损失函数。</p>
<p>举个例子，如果我们采用简单的平方误差损失函数，则我们就是要求</p>
<script type="math/tex; mode=display">
    J_{test}(\vec{w},b) = 
    \frac{1}{2m_{test}}\sum_{i=0}^{m_{test}-1} ( f_{\vec{w},b}(\vec{x}^{(i)}_{test}) - y^{(i)}_{test} )^2</script><p>$J_{test}(\vec{w},b)$ 就是我们对该模型泛化能力的衡量指标，称为测试误差。</p>
<p>我们可以将该指标与训练集的损失函数值 $J<em>{train}(\vec{w},b)$（注意这里没有正则项，称为训练误差）进行比较。一般的情况是 $J</em>{test}(\vec{w},b)$ 大于 $J_{train}(\vec{w},b)$。</p>
<p>我们还可以将该模型的泛化能力与其他模型的泛化能力，或者与人类的误差进行比较。这样我们就可以评价我们所训练的模型的优劣了。</p>
<h2 id="三、交叉验证集"><a href="#三、交叉验证集" class="headerlink" title="三、交叉验证集"></a>三、交叉验证集</h2><p>在我们继续评估模型之前，我们要先来考虑一些我们究竟要如何进行优化。你可能会想，我们既然有了评价模型泛化能力的指标 $J_{test}$，那么只要试图让该指标减小不就可以了吗？这是一个很直接的想法。</p>
<p>但问题是，如果我们向着使 $J_{test}$ 减小的目标优化，那么测试集与训练集有什么区别呢？按照这种方法，测试集就相当于变成了训练集的一部分，而不再能成为衡量训练集之外泛化能力的样本了。</p>
<p>因此，我们不能将训练集作为优化的方向，而只能将其作为评价的指标。我们需要再分出一部分数据，作为优化时的参考。这就是交叉验证（Cross-validation）集。对应的衡量指标为 $J_{cv}$</p>
<h2 id="四、模型评价"><a href="#四、模型评价" class="headerlink" title="四、模型评价"></a>四、模型评价</h2><p>我们用偏差和方差来评价一个模型。</p>
<ul>
<li>偏差表示模型对训练集的预测相对于真实情况的误差，如果模型对训练样本都不能很好地拟合或分类，就称其为高偏差的。 $J_{train}$ 可衡量模型的偏差。</li>
<li>方差表示模型对测试集的预测相对于真实情况的误差（也就是泛化能力本身），如果模型不能很好地预测测试集中数据，就称其为高方差的。$J_{cv}$ 可衡量模型的方差。</li>
</ul>
<blockquote>
<p>一般来说，模型的方差会大于偏差。因此高偏差的模型都是高方差的。</p>
<p>随着模型复杂度的提升，模型的偏差会逐渐降低，但偏差会先降低再升高；随着训练样本量的提升，偏差会逐渐减小，方差会逐渐升高，直到最后方差和偏差程度接近。</p>
</blockquote>
<h2 id="五、倾斜数据集的误差指标"><a href="#五、倾斜数据集的误差指标" class="headerlink" title="五、倾斜数据集的误差指标"></a>五、倾斜数据集的误差指标</h2><p>对于正面例子和负面例子比例差距较大的训练样本（也就是说，被标记为某一结果的样本数量占样本总数的比例很大或很小），根据全概率分布可知，可能就算全部预测同一结果，其正确的概率也很大。因此我们需要能消除样本本身概率分布导致的正确率偏差的指标。</p>
<p>有两种误差指标用于评价这种情况。分别为准确率（Precision）和召回率（Recall）。</p>
<ul>
<li>准确率衡量的是，模型预测为某一结果的情况中，有多少是预测正确的。</li>
<li>召回率衡量的是，对所有真正为某一结果的情况，有多少是模型预测到的。</li>
</ul>
<p>也就是说，设</p>
<ul>
<li>TP表示将正类预测为正类的数量</li>
<li>FN表示将正类预测为负类的数量</li>
<li>FP表示将负类预测为正类的数量</li>
<li>TN表示将负类预测为负类的数量</li>
</ul>
<p>则准确率为</p>
<script type="math/tex; mode=display">
    P = \frac{TP}{TP + FP}</script><p>召回率为</p>
<script type="math/tex; mode=display">
    R = \frac{TP}{TP + FN}</script><p>可以这样理解，把预测的过程当做选择的过程，准确率高就是“不重”，不过多的选择，以致把许多不属于的也包括在内；召回率高就是“不漏”，不过少地选择，把一些属于的漏掉了。</p>
<p>我们很容易就能得知，准确率和召回率在一定程度上是矛盾的。想要准确率高，就很可能漏掉一些；想要召回率高，就可能包含了不属于的东西。</p>
<blockquote>
<p>对于逻辑回归来说，提高阈值（即把多少数值以上的设定为真），准确率上升，召回率下降；降低阈值，准确率下降，召回率上升。</p>
</blockquote>
<p>要权衡准确率和召回率，对模型进行整体的评价的方法是采用 F1 分数（F1 Score）。即</p>
<script type="math/tex; mode=display">
    F1 = \frac{1}{\frac{1}{2}(\frac{1}{P} + \frac{1}{R})}</script><blockquote>
<p>相当于对准确率和召回率的调和平均</p>
</blockquote>
<h2 id="五、优化策略"><a href="#五、优化策略" class="headerlink" title="五、优化策略"></a>五、优化策略</h2><p>优化模型需要考虑不同的情况。</p>
<p>一种是模型具有高偏差的情况。<br>模型具有高偏差，常常意味着当前的模型复杂度不足以拟合当前数据。此时若一味地增加训练数据量，并不会使得拟合效果提高太多。应该做的是提高模型复杂度。如增加特征、增加层数、减小正则化系数等等。</p>
<p>另一种是模型具有高方差，且偏差较低的情况。<br>这种情况常常意味着模型对样本产生了过拟合。需要做的是降低拟合程度。采取如减少特征、增大正则化系数等手段。增加训练数据量在这种情况下可以起到效果。</p>
<p><strong>这里再介绍一些增加数据量的方法：</strong></p>
<p>除了通过各种手段获取新数据以外，我们还可以利用现有数据创造新数据。如对于图像，可以将其经过旋转、伸缩、扭曲等操作生成新的图像；又比如音频，可以增加背景音、噪声等来模拟更加真实的环境。这些手段都能增加模型的稳健性和泛用性。</p>
<p>迁移学习同样是一种利用数据的手段。该方法获得经过其他数据训练过的用于完成类似任务的模型，修改部分神经层（如输出层），再进行当前任务的训练。这样做相当于使用了其他任务中的数据。（这种方法可行的原因是，相似的任务，在位置靠前的部分常有类似的处理过程）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/76c44d63/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/76c44d63/" class="post-title-link" itemprop="url">机器学习之神经网络基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-14 19:25:16" itemprop="dateCreated datePublished" datetime="2022-10-14T19:25:16+08:00">2022-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、神经元模型"><a href="#一、神经元模型" class="headerlink" title="一、神经元模型"></a>一、神经元模型</h2><p>在研究人工智能的过程中，模拟生物的神经是一条很显然的道路。神经网络模型最初便是以模拟生物的神经网络为目的的。但是在早期算力不足的情况下，神经网络的效果并不怎么好。直到硬件基础成熟的最近一段时间，神经网络才发挥出超常的能力。</p>
<p>当然，此时经过改进、优化后的神经网络也在一定程度上偏离了最初模拟生物神经系统的初衷。当前的，以统计为基础的神经网络模型是否是实现强人工智能的有效方法也不得而知。但是我们还是有必要对神经网络模型，特别是作为神经网络的最小单元——神经元模型，做充分的介绍。因为事实证明了，这是一条有效的创造 AI 工具的途径。</p>
<p>单个神经元可以看做有多个输入 $x_j$，单个输出 $a$ 的函数。神经元模型具有如下属性：</p>
<ul>
<li>权重 $\vec{w}, b$</li>
<li>激发函数 $g(z)$</li>
</ul>
<p>所对应的函数为</p>
<script type="math/tex; mode=display">
    f_{\vec{w}, b, g(z)}(\vec{x}) = g(\vec{w} \cdot \vec{x} + b)</script><blockquote>
<p>取 $g(z) = \frac{1}{1 + e^{-z}}$, 与我们之前学习过的逻辑回归没有差别。</p>
</blockquote>
<p>通过对这样的神经元按一定规则进行组合，便可以构造用于完成某种功能的神经网络。</p>
<h2 id="二、神经网络层"><a href="#二、神经网络层" class="headerlink" title="二、神经网络层"></a>二、神经网络层</h2><p>对于一组输入，我们可以将其依次通过不同的神经元进行计算，计算的结果可以按顺序组成向量，作为一组新的输入。这种不断迭代的能力是神经网络的基础。我们称同一次计算使用的所有神经元为<strong>同一层</strong>的神经元。这些神经元组成了神经网络上的一<strong>层</strong>。不同层的组合构成了整个神经网络。</p>
<blockquote>
<p>注意，同一层的神经元需要满足输入个数相同、激发函数相同。</p>
</blockquote>
<p>从数学上讲，设输入为 $\vec{x} = (x_1, x_2, …, x_n)$，输出为 $\vec{a} = (a_1, a_2, …, a_m)$，神经元函数为 $f_1(\vec{x}), f_2(\vec{x}), … f_m(\vec{x})$ 则</p>
<script type="math/tex; mode=display">
    \vec{a} = (f_1(\vec{x}), f_2(\vec{x}), ... f_m(\vec{x}))</script><p>对于不同层上的属性或参数 x，我们加上上标 $x^{[i]}$ 表示该属性或参数属于第 i 层（第几层指示神经元计算的顺序）；对于同一层的不同神经元，加上下标 $x^{[i]}_j$ 表示第 i 层的第 j 个神经元的属性或参数。</p>
<p>则更一般的，假设第 i 层接受 n 个参数的输入，并具有 m 个神经元，则第 i 层上的运算可表示为：</p>
<script type="math/tex; mode=display">
        \left\{
        \begin{array}{lr}
            a^{[i]}_1 = g(\vec{w}^{[i]}_1 \cdot \vec{a}^{[i-1]} + b^{[i]}_1) \\
            a^{[i]}_2 = g(\vec{w}^{[i]}_2 \cdot \vec{a}^{[i-1]} + b^{[i]}_2) \\
            \vdots \\
            a^{[i]}_m = g(\vec{w}^{[i]}_m \cdot \vec{a}^{[i-1]} + b^{[i]}_m)
        \end{array}
    \right.</script><blockquote>
<p>$\vec{a}^{[0]}$ 表示神经网络的总输入</p>
</blockquote>
<h2 id="三、神经网络的矩阵表示"><a href="#三、神经网络的矩阵表示" class="headerlink" title="三、神经网络的矩阵表示"></a>三、神经网络的矩阵表示</h2><p>神经网络所以一种有效的机器学习方法，在于其清晰的结构和高效的计算方式。清晰的结构已经在上一部分有所说明，这里进一步阐述神经网络的矩阵表示形式。通过矩阵并行化计算，神经网络的运行速度将得到极大地提升。</p>
<p>对于单一神经元上的权重，我们可以将其表示成列向量的形式，即</p>
<script type="math/tex; mode=display">
    W^{[i]}_j = \begin{bmatrix}
        w_1 & w_2 & \cdots & w_n
    \end{bmatrix}^T</script><p>按照上一节中的公式，要进行点乘运算，则应该将输入表示为行向量的形式</p>
<script type="math/tex; mode=display">
    A^{[i-1]} = \begin{bmatrix}
        a_1 & a_2 & \cdots & a_n
    \end{bmatrix}</script><p>则 $\vec{w}^{[i]}_j \cdot \vec{a}^{[i-1]}$ 就可以转化为 $A^{[i-1]}W^{[i]}_j$。</p>
<p>之后，我们可以先忽略激发函数。只看激发函数内的一组线性表达式，将这组表达式的结果也按顺序表示为行向量</p>
<script type="math/tex; mode=display">
    Z^{[i]} = \begin{bmatrix}
        z^{[i]}_1 & z^{[i]}_2 & \cdots & z^{[i]}_m
    \end{bmatrix}</script><p>把 b 的部分提出单独作为一个向量，记</p>
<script type="math/tex; mode=display">
    B^{[i]} = \begin{bmatrix}
        b_1 & b_2 & \cdots & b_m
    \end{bmatrix}</script><p>则</p>
<script type="math/tex; mode=display">
    Z^{[i]} = \begin{bmatrix}
        A^{[i-1]}W^{[i]}_1 & A^{[i-1]}W^{[i]}_2 & \cdots & A^{[i-1]}W^{[i]}_m
    \end{bmatrix} + B^{[i]}</script><p>再将输入 $A^{[i-1]}$ 提出来</p>
<script type="math/tex; mode=display">
    Z^{[i]} = A^{[i-1]} \begin{bmatrix}
        W^{[i]}_1 & W^{[i]}_2 & \cdots & W^{[i]}_m
    \end{bmatrix} + B^{[i]}</script><p>此时各个神经元的权重组合成了一个新的矩阵，记</p>
<script type="math/tex; mode=display">
    W^{[i]} = \begin{bmatrix}
        W^{[i]}_1 & W^{[i]}_2 & \cdots & W^{[i]}_m
    \end{bmatrix}_{n \times m}</script><blockquote>
<p>可以认为，神经网络的层的本质就是矩阵 $W^{[i]}$ 和向量 $B^{[i]}$</p>
</blockquote>
<p>此时如果我们认为函数 $g(Z)$ 表示对矩阵中的每一个元素应用激活函数 $g(z)$ ，则整个神经网络层就可以表示为</p>
<script type="math/tex; mode=display">
    A^{[i]} = g(A^{[i-1]} W^{[i]} + B^{[i]})</script><p>这不只是形式上的化简。利用 numpy 等数学库，可以极大加快运算的速度。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/757349f6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/757349f6/" class="post-title-link" itemprop="url">系统编程之shell编程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-08 19:30:58" itemprop="dateCreated datePublished" datetime="2022-10-08T19:30:58+08:00">2022-10-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本文将简单探索shell脚本编程，介绍shell的基本语法。</p>
<h2 id="二、shell简介"><a href="#二、shell简介" class="headerlink" title="二、shell简介"></a>二、shell简介</h2><p>shell是一个命令解释器，可以用来启动、停止、编写程序；是用户和UNIX/Linux操作系统内核程序间的一个接口。</p>
<p>而shell编程则是将linux命令与shell的各种流程控制和条件判断来组合成命令与变量，形成可以进行自动处理的脚本程序。</p>
<h2 id="三、前期准备"><a href="#三、前期准备" class="headerlink" title="三、前期准备"></a>三、前期准备</h2><h3 id="创建脚本"><a href="#创建脚本" class="headerlink" title="创建脚本"></a>创建脚本</h3><p>shell脚本是一个文本文件，可用文本编辑器如vi、vim编辑保存。创建shell脚本只需按照创建文本文件的方式创建。如<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi c1.sh</span><br><span class="line">vim c2.sh</span><br><span class="line">&gt; c3.sh</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>shell脚本一般以<code>.sh</code>为后缀，但没有后缀依旧可以执行。</p>
</blockquote>
<p>创建的shell脚本，一定要在开头第一行加上如下语句：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br></pre></td></tr></table></figure><br>这一行将指明该脚本执行所需要的命令解释器。</p>
<h3 id="执行脚本"><a href="#执行脚本" class="headerlink" title="执行脚本"></a>执行脚本</h3><p>shell脚本的执行方法有<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sh &lt;scriptname&gt;</span><br><span class="line">bash &lt;scriptname&gt;</span><br></pre></td></tr></table></figure><br>或者使用chmod命令修改脚本为可执行，再直接使用 <code>./&lt;scriptname&gt;</code> 运行。</p>
<h2 id="四、基本语法"><a href="#四、基本语法" class="headerlink" title="四、基本语法"></a>四、基本语法</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>shell中的变量分为环境变量、用户定义变量、内部变量。</p>
<p>其中环境变量是操作系统的一部分，但可以利用shell脚本进行修改；用户变量即在脚本中声明的变量；而内部变量则用来指示脚本运行中出现的一些变量。</p>
<h4 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h4><p>只有用户变量可以声明。和其他语言一样，使用等号进行声明。但要注意的是，shell脚本是弱类型的，因此变量名前不需要加上类型名。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var=hello_world</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意shell对空格敏感，声明时等号两边不能有空格</p>
</blockquote>
<p>另外可以在变量名前添加 <code>readonly</code>关键字设为只读<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">readonly constVar</span><br></pre></td></tr></table></figure></p>
<p>shell中声明数组同样直接写出数组名称<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr[0]=1</span><br><span class="line">arr[1]=5</span><br><span class="line">arr[10]=20</span><br></pre></td></tr></table></figure><br>未赋值的部分默认为NULL。</p>
<blockquote>
<p>注意 ubuntu 默认使用 dash 而非 bash shell。dash 并不支持数组。要使用数组可以用bash运行脚本，即运行命令 <code>bash &lt;scriptname&gt;</code></p>
</blockquote>
<h4 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h4><p>shell变量有类似左值右值的区别。在用其他变量进行赋值时，需要对变量使用<code>$&#123; &#125;</code> 进行取值。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var1=hi</span><br><span class="line">var2=$&#123;var1&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="引号"><a href="#引号" class="headerlink" title="引号"></a>引号</h4><p>shell脚本是为了自动化处理命令而设计的。因此语法中有很大一部分关注于字符串和命令的相关操作。在变量上体现在，shell中所有变量默认以字符串形式存在。</p>
<p>并且，为了满足命令处理的需要，shell设计出了引号变量值。</p>
<p>shell中的引号包括单引号、双引号和倒引号。</p>
<p><strong>单引号</strong>中的字符均作为普通字符出现。（可以包括空格）<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var1=hello_world</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">var2=hello world <span class="comment"># 不合法</span></span></span><br><span class="line">var3=&#x27;hello world&#x27;</span><br></pre></td></tr></table></figure></p>
<p><strong>双引号</strong>中的字符大部分作为普通字符对待。除了<code>$\’</code>和双引号，这些变量依旧用于对字符串内容进行变量替换。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">var4=hello</span><br><span class="line">var5=friend</span><br><span class="line">var6=&quot;$var4, my $var5!&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">var6 means <span class="string">&quot;hello, my friend!&quot;</span>.</span></span><br></pre></td></tr></table></figure></p>
<p><strong>倒引号</strong>将引号内的内容当做命令，会先执行倒引号内的内容，再用执行后的输出替换倒引号的内容。</p>
<blockquote>
<p>倒引号可以和双引号组合使用，在双引号内使用倒引号<br>$(command) 与 `command` 功能相同</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var7=&quot;now pwd: `pwd`&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">var6 means <span class="string">&quot;now pwd: /home/username&quot;</span> (depend on the position <span class="built_in">where</span> you execute the script)</span></span><br></pre></td></tr></table></figure>
<h3 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h3><h4 id="字符串操作"><a href="#字符串操作" class="headerlink" title="字符串操作"></a>字符串操作</h4><p>shell脚本自身具有一些字符串操作功能。</p>
<ul>
<li>字符串长度：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;<span class="comment">#str&#125;</span></span></span><br></pre></td></tr></table></figure></li>
<li>字符串截取：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str:position&#125;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str:start:length&#125;</span></span><br></pre></td></tr></table></figure></li>
<li>从字符串开头删除：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str<span class="comment">#substr&#125;</span></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str<span class="comment">##substr&#125;</span></span></span><br></pre></td></tr></table></figure></li>
<li>从字符串末尾删除：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str%substr&#125;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str%%substr&#125;</span></span><br></pre></td></tr></table></figure></li>
<li>字符串替换：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;str/substr/replace&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>字符串操作中出现匹配的部分都支持正则表达式。其中一个字符的为懒惰匹配，两个字符的为贪婪匹配。</p>
<p>使用 <code>expr</code> 等命令，可以更好地进行处理。</p>
</blockquote>
<h4 id="数字运算"><a href="#数字运算" class="headerlink" title="数字运算"></a>数字运算</h4><p>shell默认变量为字符串，因此若要进行数字运算，需要特殊指明。</p>
<p>具体地，有两种方法。一种是利用 <code>$(())</code> 运算符。指明括号内的表达式进行的不是字符串操作而是数字运算。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num1=10</span><br><span class="line">num2=20</span><br><span class="line">num3=$(($num1+$num2)) # 30</span><br></pre></td></tr></table></figure></p>
<p>另一种方法是调用 linux 的 expr 命令。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num4=3</span><br><span class="line">num5=$num4</span><br><span class="line">num6=`expr $num4 \* $num5` # 9</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意乘号在shell脚本中有特殊含义，因此需要加入反斜杠转义。另外 expr 命令中变量和运算符各自作为参数，因此中间需要用空格隔开.</p>
</blockquote>
<h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><h4 id="条件表达式"><a href="#条件表达式" class="headerlink" title="条件表达式"></a>条件表达式</h4><p>shell 利用 <code>test condition</code> 和 <code>[condition]</code> 进行条件测试。其中“condition” 表示一个条件表达式。包括字符比较、数值比较、文件操作、逻辑操作。</p>
<p>这一部分原理较为简单，但shell中判断符号与其他语言有较大差别，内容较多，就不一一列举了。</p>
<blockquote>
<p>注意使用条件表达式时，表达式与方括号间要有空格隔开</p>
</blockquote>
<h4 id="分支控制"><a href="#分支控制" class="headerlink" title="分支控制"></a>分支控制</h4><p>分支控制语句如下</p>
<p>if 分支：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if [ condition ]</span><br><span class="line">then</span><br><span class="line">    commands</span><br><span class="line">elif</span><br><span class="line">    commands</span><br><span class="line">else</span><br><span class="line">    commands</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意末尾的 fi</p>
</blockquote>
<p>case分支（类似switch）：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">case variable in</span><br><span class="line">var1)</span><br><span class="line">    commands</span><br><span class="line">    ;;</span><br><span class="line">var2 | var3)</span><br><span class="line">    commands</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    commands</span><br><span class="line">esac</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意两个连续的分号，类似于 break。进入同样分支的变量值要用 “|” 隔开。末尾要添加 esac。</p>
</blockquote>
<h4 id="循环控制"><a href="#循环控制" class="headerlink" title="循环控制"></a>循环控制</h4><p>循环控制语句如下</p>
<p>while 循环：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while [ condition ]</span><br><span class="line">do</span><br><span class="line">    commands</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p>
<p>until 循环：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">until [ condition ]</span><br><span class="line">do</span><br><span class="line">    commands</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>until 循环与条件取反的 while 循环等价</p>
</blockquote>
<p>for 循环如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for arg in args</span><br><span class="line">do</span><br><span class="line">    commands</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>其中 args 是一组列出的变量名或字符串</p>
</blockquote>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="函数的定义"><a href="#函数的定义" class="headerlink" title="函数的定义"></a>函数的定义</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function func_name&#123;</span><br><span class="line">    commands</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func_name()&#123;</span><br><span class="line">    commands</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>两种方式均可定义函数。</p>
<h4 id="参数取值"><a href="#参数取值" class="headerlink" title="参数取值"></a>参数取值</h4><p>shell中的函数只有可变参数。另外，整个shell脚本本身也可以看做一个函数，它同样也只有可变参数。都具有相同的取参数的方式。</p>
<p>shell中通过 <code>$0 $1 $2 ... $9</code>等参数来获取函数对应位置的参数。其中 <code>$0</code> 最开始为函数名/脚本名。当参数多于 10 个时，要获取十个之外的参数，可以使用 <code>shift</code> 命令，使得除了函数名/脚本名 $0 之外的所有参数整体左移一位。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sh &lt;scriptname&gt;.sh 123 456</span></span><br><span class="line">echo $1 # 123</span><br><span class="line">shift</span><br><span class="line">echo $1 # 456</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><code>echo</code> 为输出命令</p>
</blockquote>
<h4 id="函数返回值"><a href="#函数返回值" class="headerlink" title="函数返回值"></a>函数返回值</h4><p>函数通过 <code>return [n]</code> 语句来返回变量值 n。如果没有设置返回值，那么会默认返回函数最后一条命令执行后的返回值。</p>
<p>在函数调用之后，可以使用 <code>$?</code> 获取函数的返回值。</p>
<blockquote>
<p>注意 return 的值必须只能是 0~255间的整数。要返回字符串或者更大的数字，可以直接使用 <code>echo</code> 输出内容，再通过倒引号获取输出的内容。</p>
</blockquote>
<h4 id="函数实例"><a href="#函数实例" class="headerlink" title="函数实例"></a>函数实例</h4><p>编写一个递归计算阶乘的函数<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">frac()</span><br><span class="line">&#123;</span><br><span class="line">    if [ $(($1)) -ge 1 ]</span><br><span class="line">    then</span><br><span class="line">        echo $((`frac $(($1-1))`*$1))</span><br><span class="line">    else</span><br><span class="line">        echo 1</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo `frac 5` # 120</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/6d380668/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/6d380668/" class="post-title-link" itemprop="url">机器学习笔记之正则化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-08 19:27:57" itemprop="dateCreated datePublished" datetime="2022-10-08T19:27:57+08:00">2022-10-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、前言——过拟合"><a href="#一、前言——过拟合" class="headerlink" title="一、前言——过拟合"></a>一、前言——过拟合</h2><p>我们知道，经过一组点可以有无数条曲线。这些曲线对于这组样本点的损失函数同为 0。但是对于预测来说，这些曲线产生的结果却并不相同。这就意味着，进行梯度下降到达某一最低点时，依旧不一定能得到“最好的”预测（拟合/分类）效果。甚至可能对于一些情况，此时的（预测/分类）效果反而更差了。这样的情况就称为过拟合。</p>
<p>过拟合的存在是很合理的。从感性上讲，将机器学习的过程类比人类的认知，一个观念的形成不能超出经验之外，认知的结果永远是片面而非客观的。那么在与更广泛的客观现实接触之前，我们必然无法得知已经形成的认知是否是依旧可以应用的。</p>
<p>这是一个很休谟的观点。但却无法解决现实问题。我们依旧需要找到减少过拟合的方法。</p>
<h2 id="二、惩罚"><a href="#二、惩罚" class="headerlink" title="二、惩罚"></a>二、惩罚</h2><p>我们的思想中存在着一种先验观念，它规范天地万物，在冥冥中告诉我们什么是“合理的”。对于机器学习模型来说也是一样的，它应当具有这样的机制，告诉它什么情况是不可能的。</p>
<p>就比如说，对于房价，我们知道一些特征是更加重要的，而另一些是更加不重要的。很显然，那些更重要的特征对应的权重应该较不重要的特征对应的权重大。那么我们就需要对那些不重要的权重进行“惩罚”，以避免这些权重过大，从而导致模型过拟合。这样的“惩罚”在损失函数中体现。即，当这些权重过大时，损失函数也会相应增大。</p>
<p>具体而言，对如下的表达式</p>
<script type="math/tex; mode=display">
    y = w_1x_1 + w_2x_2 + w_3x_3 + b</script><p>假设要使第二、三个权重相对较小，则可以在损失函数中加上惩罚项 $\lambda_2 w_2 + \lambda_3 w_3$。其中 $\lambda_2, \lambda_3$ 取较大值。则损失函数变为</p>
<script type="math/tex; mode=display">
    J_{new}(\vec{w}, b) = J(\vec{w}, b) + \lambda_2 w_2 + \lambda_3 w_3</script><p>具体地比如说</p>
<script type="math/tex; mode=display">
    J_{new}(\vec{w}, b) = J(\vec{w}, b) + 1000 w_2 + 2000 w_3</script><p>那么此时很显然，当 $w_2, w_3$ 较大时，损失函数也会相应更大。</p>
<p>可是对于大多数情况，我们无法事先知晓权重的重要程度。对于这些一般化的问题，还需要有一般化的解决办法。</p>
<h2 id="三、正则化"><a href="#三、正则化" class="headerlink" title="三、正则化"></a>三、正则化</h2><p>正则化是惩罚的一种。该方法在损失函数中增加了正则项：</p>
<script type="math/tex; mode=display">
    \frac{\lambda}{2m} \sum_{j=1}^n w^2_j</script><p>其中 $\lambda$ 称为正则化参数。</p>
<p>则新的损失函数为</p>
<script type="math/tex; mode=display">
    J_{new}(\vec{w}, b) = J(\vec{w}, b) + \frac{\lambda}{2m} \sum_{j=1}^n w^2_j</script><p>这里有几个值得解释的地方：</p>
<h3 id="惩罚所有权重"><a href="#惩罚所有权重" class="headerlink" title="惩罚所有权重"></a>惩罚所有权重</h3><p>第一是<strong>对所有权重的惩罚</strong>。如果按照上一节的解释，很容易认为，正则化等同于对所有权重的相同比例的惩罚。这是否相当于对所有权重都不惩罚呢？若是如此，那么正则化是否还有意义？</p>
<p>关键在于正则项的 $w^2_j$ 上。这是权重的平方而非权重本身。我们对损失函数求 $w_j$ 的偏导。</p>
<script type="math/tex; mode=display">
    \frac{\partial{J_{new}(\vec{w}, b)}}{\partial{w_j}}
    = \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}}
    + \frac{\lambda}{m} w_j</script><p>我们可以得到两个结论。<br>首先，正则项永远使权重不断趋于0。即便在到达或接近原损失函数最小值时，$\frac{\partial{J(\vec{w}, b)}}{\partial{w_j}} \approx 0$，但此时正则项依旧会发挥作用。这就使得权重无法安定在拟合效果最好的位置。</p>
<p>并且，权重（绝对值）更大的特征，其权重值趋近于 0 的“速度”要快于权重更小的特征。这就避免了对所有权重同一的惩罚。</p>
<p>由此我们也可以看出正则项“眼中”最好的权重是什么样子的。那就是所有权重均为 0。当然，通过合理选择学习率 $\alpha$ 和正则化参数 $\lambda$，是不可能让这种情况发生的。但正则项却可以实实在在地提供一种“拉力”，用来纠正过于“复杂”的模型结构，使得拟合得到的超平面倾向于平缓。</p>
<h3 id="除-m-缩放"><a href="#除-m-缩放" class="headerlink" title="除 m 缩放"></a>除 m 缩放</h3><p>第二需要解释的是<strong>正则项中的除 m 缩放</strong>。这样做的目的是保证同一正则化参数在不同的样本数量下都有效。</p>
<p>我们以正则化了的多元线性回归损失函数对 $w_j$ 偏导为例。</p>
<script type="math/tex; mode=display">
    \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}}
    = \frac{1}{m}\sum_{i=1}^{m}(\vec{w}\cdot\vec{x}^{(i)} + b - y^{(i)})x^{(i)}_j
    + \frac{\lambda}{m} w_j</script><p>假设正则项并不除以 m，或者按照感觉，除以 n。那么在学习率和正则化参数不变的情况下，随着样本量的增加，梯度下降时正则化对权重的影响保持不变；可原损失函数却因为除以了m，对权重的影响减小。这样就会影响正则化参数在不同样本量时的有效性。</p>
<p>因此对正则项也采取除 m。这样在样本量增加时，原损失函数和正则项对权重的影响就是同比例减小的了。</p>
<h2 id="四、代码实现"><a href="#四、代码实现" class="headerlink" title="四、代码实现"></a>四、代码实现</h2><p>这一部分很简单，就不具体写出来了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/ba0d6023/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/ba0d6023/" class="post-title-link" itemprop="url">机器学习笔记之逻辑回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-05 16:06:34" itemprop="dateCreated datePublished" datetime="2022-10-05T16:06:34+08:00">2022-10-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、前言——逻辑分类"><a href="#一、前言——逻辑分类" class="headerlink" title="一、前言——逻辑分类"></a>一、前言——逻辑分类</h2><p>机器学习研究的另一种问题为分类问题。给出一些信息，判断是或不是某种物体、属性。比如说给出病人的各项检测指标，判断其是否患病。</p>
<p>对于是、否的判断，这里用 1、0 表示。则训练样本就与之前的线性与多项式回归样本类似。</p>
<p>我们可以试着用一元线性回归来拟合分类问题的样本。</p>
<p>比如对于如下的样本数据<br><img src="/posts/ba0d6023/scatter1.png" class="" title="scatter1"></p>
<p>进行一元线性回归后得到<br><img src="/posts/ba0d6023/linear1.png" class="" title="linear1"></p>
<p>我们将数值大于 0.5 的部分看做预测为真，小于 0.5 的部分看做预测为假。可以看到，此时一元线性回归就已经可以使所有样本点符合判断结果了。</p>
<p>但如果我们在 x 值更大的区域增加更多的样本点，再进行回归<br><img src="/posts/ba0d6023/linear2.png" class="" title="linear2"></p>
<p>可以看到，有一部分应为 1 的点被分在了 y &lt; 0.5 的部分，被预测为 0。这说明只凭线性回归无法解决分类问题。我们需要另一种回归方法。</p>
<h2 id="二、逻辑回归"><a href="#二、逻辑回归" class="headerlink" title="二、逻辑回归"></a>二、逻辑回归</h2><p>线性回归失效的原因在于，拟合的目标是对所有点的方差最小，在分类边界上的样本可能因为其他样本的影响而被分到另一类。而为什么会受到其他样本的影响呢？因为线性回归得到的表达式值域趋向于无穷；而样本的结果却只有 0、1。所以对于一个样本，只要它的特征数值足够大或足够小，就足以产生极大的损失。（为什么可能出现足够远的样本？因为分类问题需要划分出一个边界，在边界两侧可能存在足够大的范围。）</p>
<p>我们要对线性回归进行修改，需要将回归得到的表达式的值域缩小到 0 至 1 的范围。</p>
<p>这里引入 Sigmoid 函数</p>
<script type="math/tex; mode=display">
    S(z) = \frac{1}{1 + e^{-z}}</script><p>它的图像如下<br><img src="/posts/ba0d6023/sigmoid.png" class="" title="sigmoid"></p>
<p>该函数定义域为 $(-\infty, +\infty)$，值域为 $(0, 1)$</p>
<p>假设原本线性回归的函数为</p>
<script type="math/tex; mode=display">
    f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b</script><p>则现在令</p>
<script type="math/tex; mode=display">
    f_{\vec{w}, b}(\vec{x}) = S(\vec{w} \cdot \vec{x} + b)</script><p>即</p>
<script type="math/tex; mode=display">
    f_{\vec{w}, b}(\vec{x}) = \frac{1}{1 + e^{\vec{w} \cdot \vec{x} + b}}</script><p>为新的函数。其中 $S(z)$ 即为 Sigmoid 函数。这样，表达式的值域就缩小为了 $(0, 1)$，从而消除了值域对损失的影响。</p>
<h2 id="三、损失函数"><a href="#三、损失函数" class="headerlink" title="三、损失函数"></a>三、损失函数</h2><p>如果我们依旧采用线性回归时的损失函数计算方式，就会发现函数非凸，那么梯度下降法将不能得到很好的结果。因此我们重新定义损失函数。</p>
<p>设单样本损失函数</p>
<script type="math/tex; mode=display">
    loss(f_{\vec{w},b}(\vec{x}^{(i)}), y^{(i)}) = \begin{cases}
        -log(f_{\vec{w},b}(\vec{x}^{(i)}))  &  \text{if } y^{(i)} = 1 \\
        -log(1 - f_{\vec{w},b}(\vec{x}^{(i)})) & \text{if } y^{(i)} = 0
    \end{cases}</script><p>这个损失函数保证了在样本真实结果为真或为假的条件下都能以同样的标准衡量预测的误差。</p>
<p>另外，因为 $y^{(i)}$ 的取值只有 0、1。所以可以将上式化简为单个公式的形式</p>
<script type="math/tex; mode=display">
    loss(f_{\vec{w},b}(\vec{x}^{(i)}), y^{(i)}) = 
    -y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - y^{(i)})log(1 - f_{\vec{w},b}(\vec{x}^{(i)}))</script><p>则全样本代价函数</p>
<script type="math/tex; mode=display">
    J(\vec{w}, b) = \frac{1}{m} \sum_{i = 1}^m loss(f_{\vec{w},b}(\vec{x}^{(i)}), y^{(i)})</script><h2 id="四、梯度下降"><a href="#四、梯度下降" class="headerlink" title="四、梯度下降"></a>四、梯度下降</h2><p>我们对损失函数求导，得</p>
<script type="math/tex; mode=display">
    \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}}
    = \frac{1}{m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)}) x^{(i)}_j \\
    \frac{\partial{J(\vec{w}, b)}}{\partial{b}}
    = \frac{1}{m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})</script><p>可以发现求导后的公式形式与线性回归时的相同，这是有意为之的。但是要注意的是， $f_{\vec{w},b}(\vec{x}^{(i)})$ 在线性回归与逻辑回归时并不相同。逻辑回归时的表达式是线性回归时的加上 Sigmoid 函数。</p>
<p>按照与线性回归相同的方法，以确定的步长 $\alpha$ 进行梯度下降，不断迭代</p>
<script type="math/tex; mode=display">
    w_j = w_j - \alpha \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}} \\
    b = b - \alpha \frac{\partial{J(\vec{w}, b)}}{\partial{b}}</script><p>最终便可找到使损失函数最小的 $\vec{w}, b$。</p>
<h2 id="五、代码实现"><a href="#五、代码实现" class="headerlink" title="五、代码实现"></a>五、代码实现</h2><p>在线性回归的基础上，修改损失函数的计算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_loss</span>(<span class="params">actual_y, y</span>):</span><br><span class="line">    <span class="keyword">return</span> -y * math.log(actual_y) - (<span class="number">1</span> - y) * math.log(<span class="number">1</span> - actual_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cost</span>(<span class="params">w, b, X, y</span>):</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    sum_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z = np.dot(X[i], w) + b</span><br><span class="line">        actual_y = sigmoid(z)</span><br><span class="line">        sum_cost += get_loss(actual_y, y[i])</span><br><span class="line">    total_cost = sum_cost / m</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure></p>
<p>修改梯度的计算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_gradient</span>(<span class="params">w, b, X, y</span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line"></span><br><span class="line">    sum_w = np.zeros((n, ))</span><br><span class="line">    sum_b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        err = sigmoid(np.dot(w, X[i]) + b) - y[i]</span><br><span class="line">        sum_b += err</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sum_w[j] += X[i,j] * err</span><br><span class="line"></span><br><span class="line">    gradients_w = sum_w / m</span><br><span class="line">    gradient_b = sum_b / m</span><br><span class="line">    <span class="keyword">return</span> gradients_w, gradient_b</span><br></pre></td></tr></table></figure></p>
<p>拟合结果<br><img src="/posts/ba0d6023/logical.png" class="" title="logical"></p>
<p>损失函数<br><img src="/posts/ba0d6023/cost.png" class="" title="cost"></p>
<p>对样本的预测<br><img src="/posts/ba0d6023/predict.png" class="" title="predict"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/dfc6ce5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/dfc6ce5/" class="post-title-link" itemprop="url">机器学习笔记之多项式回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-01 21:52:38" itemprop="dateCreated datePublished" datetime="2022-10-01T21:52:38+08:00">2022-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、引言——拟合多项式"><a href="#一、引言——拟合多项式" class="headerlink" title="一、引言——拟合多项式"></a>一、引言——拟合多项式</h2><p>考虑如图所示的样本数据：<br><img src="/posts/dfc6ce5/poly.png" class="" title="poly"></p>
<p>如果我们用一元线性回归去拟合该数据，会发现直线与数据点拟合效果很差。<br><img src="/posts/dfc6ce5/linearTry.png" class="" title="linearTry"></p>
<p>通过观察，我们很容易得知数据点满足二次函数，具体地，函数是 $\frac{1}{2}x^2 + 5$。既然用直线 $y = wx + b$ 不能拟合，那么换用二次函数 $y = w_1x^2 + w_2x + b$ 拟合的话效果会如何？</p>
<h2 id="二、多项式回归的步骤"><a href="#二、多项式回归的步骤" class="headerlink" title="二、多项式回归的步骤"></a>二、多项式回归的步骤</h2><p>对于一元多项式函数</p>
<script type="math/tex; mode=display">
    y = w_1x + w_2x^2 + \cdots + w_nx^n + b</script><p>令 $x^i = y_i$，则原函数变为</p>
<script type="math/tex; mode=display">
    y = w_1y_1 + w_2y_2 + \cdots + w_ny_n + b</script><p>对于自变量 $x$，只需将 $x^2, x^3, \cdots, x^n$ 看成与 $x$ 不同的特征。此时一个一元多项式函数的拟合问题就转变成了我们已知的多元线性拟合问题。同理，对于多元多项式函数，也可以将其转化为多元线性函数进行拟合。</p>
<h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><p>对于如下样本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array(<span class="built_in">range</span>(-<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">y = (x ** <span class="number">2</span>) / <span class="number">2</span> + <span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>将 $x^2$ 也作为一个特征<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.c_[x.T, (x ** <span class="number">2</span>).T] <span class="comment"># np.c_ 用于进行列的扩展</span></span><br><span class="line">y_train = y</span><br></pre></td></tr></table></figure></p>
<p>剩下的与多元线性回归同理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">1e-3</span></span><br><span class="line">w = np.zeros(X_train.shape[<span class="number">1</span>])</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    w, b = gradient_descent(w, b, X_train, y_train, alpha)</span><br></pre></td></tr></table></figure></p>
<p>迭代信息：<br><img src="/posts/dfc6ce5/print.png" class="" title="print"></p>
<p>拟合效果：<br><img src="/posts/dfc6ce5/result.png" class="" title="result"></p>
<p>损失函数：<br><img src="/posts/dfc6ce5/cost.png" class="" title="cost"></p>
<hr>
<p><strong>扩展：</strong></p>
<p>对于较为复杂的表达式，如 $y = sinx$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array(<span class="built_in">range</span>(-<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">y = np.sin(x)</span><br></pre></td></tr></table></figure></p>
<p>只要增加多项式次数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.c_[x.T, (x**<span class="number">2</span>).T, (x**<span class="number">3</span>).T, (x**<span class="number">4</span>).T, (x**<span class="number">5</span>).T, (x**<span class="number">6</span>).T, (x**<span class="number">7</span>).T, (x**<span class="number">8</span>).T, (x**<span class="number">9</span>).T, (x**<span class="number">10</span>).T, (x**<span class="number">11</span>).T]</span><br></pre></td></tr></table></figure></p>
<p>再加上特征缩放<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, _, _ = featureScaling(X_train)</span><br></pre></td></tr></table></figure></p>
<p>在一定的步长和迭代次数下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50000</span>):</span><br><span class="line">    w, b = gradient_descent(w, b, X_train, y_train, alpha)</span><br></pre></td></tr></table></figure></p>
<p>同样可以取得一定的拟合效果<br><img src="/posts/dfc6ce5/complex.png" class="" title="complex"></p>
<img src="/posts/dfc6ce5/complex_cost.png" class="" title="complex_cost">
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/9d18f94a/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/9d18f94a/" class="post-title-link" itemprop="url">机器学习笔记之特征缩放</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-01 17:16:30" itemprop="dateCreated datePublished" datetime="2022-10-01T17:16:30+08:00">2022-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、引言——参数数值对权重的影响"><a href="#一、引言——参数数值对权重的影响" class="headerlink" title="一、引言——参数数值对权重的影响"></a>一、引言——参数数值对权重的影响</h2><p>考虑有两个特征的房价预测。其一为房子面积，其二为卧室数量。参数的范围为</p>
<script type="math/tex; mode=display">
    x_1 \in [300, 2000] \\
    x_2 \in [0, 5]</script><p>预测直线为</p>
<script type="math/tex; mode=display">
    y = w_1x_1 + w_2x_2 + b</script><p>从感性上认知，如果 $w_1$ 的数值大于 $w_2$，那么由特征 $x_1$ 贡献的房价就会远大于特征 $x_2$。因为这样的话，对于同等数值的变化，特征 $x_1$ 的贡献变化大于 $x_2$，而 $x_1$ 的范围又更大，则其对贡献的变化也会更加得大。这是很不合常理的。因此从感性上讲，$w_1$ 的数值应该小于 $w_2$。</p>
<p>另外，根据损失函数的定义</p>
<script type="math/tex; mode=display">
    J(\vec{w}, b)  = \frac{1}{2m}\sum_{i=1}^{m}(\vec{w}\cdot\vec{x}^{(i)} + b - y^{(i)})^2</script><p>当损失函数值固定时，改变权重 $w_1$, 则对应的权重 $w_2$ 的变化要大于 $w_1$。从图像来说，此时损失函数形成陡峭的“山谷”，它的等高线图类似于椭圆。这时进行梯度下降，在步长较大时可能出现在“崖壁”上来回跳跃的情况。</p>
<img src="/posts/9d18f94a/contours.png" class="" title="contours">
<p>为了避免这种情况，我们需要找到一种优化的方法。</p>
<h2 id="二、特征缩放"><a href="#二、特征缩放" class="headerlink" title="二、特征缩放"></a>二、特征缩放</h2><p>我们要做的是避免出现不同特征的范围差距较大的情况。那么对于极差较大的特征，我们需要将其数值所在的区间范围缩小。这就是特征放缩。</p>
<p>在进行过特征放缩后，损失函数从图像上看将会较原来更接近正圆形，这样的话，寻找通向最低点的路径将会更加容易。</p>
<p>特征缩放有许多不同的方法，如：</p>
<ol>
<li>除数特征缩放</li>
<li>均值归一化（Mean normalization）</li>
<li>Z-score标准化（Z-score normalization）</li>
</ol>
<p>除数特征缩放指的是将该特征的所有值同除以某一个数，比如说最大值。</p>
<script type="math/tex; mode=display">
    x_{scaled} = \frac{x}{x_{max}}</script><p>均值归一化是以均值为参照对所有数值进行缩放，公式：</p>
<script type="math/tex; mode=display">
    x_{scaled} = \frac{x - x_{mean}}{x_{max} - x_{min}}</script><p>Z-score标准化将数值转换为正态分布。公式：</p>
<script type="math/tex; mode=display">
    x_{scaled} = \frac{x - \mu}{\sigma}</script><p>其中</p>
<script type="math/tex; mode=display">
    \mu = \frac{1}{m}\sum_{i = 1}^m x^{(i)} \\
    \sigma^2 = \frac{1}{m}\sum_{i = 1}^m (x^{(i)} - \mu)^2</script><h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><p>利用 Z-score 标准化对特征进行缩放<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">featureScaling</span>(<span class="params">X</span>):</span><br><span class="line">    mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># mean 方法求平均值，其中 axis 参数指定对哪一个维度求平均。</span></span><br><span class="line"></span><br><span class="line">    sigma = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># std 方法用来求标准差</span></span><br><span class="line"></span><br><span class="line">    X_scaled = (X - mu) / sigma</span><br><span class="line">    <span class="comment"># 直接利用运算符，X中的每一组样本，减去 mu， 除以 sigma。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_scaled, mu, sigma</span><br></pre></td></tr></table></figure></p>
<p>多元线性回归代码，在初始化 <code>X_train</code> 变量的代码后添加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, mu, sigma = featureScaling(X_train)</span><br></pre></td></tr></table></figure></p>
<p>并将步长改为 0.01（$0.01 \gg 1 \times 10^{-7}$ ！！！）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.01</span></span><br></pre></td></tr></table></figure></p>
<p>因为采用的是经过缩放后的特征值，所以回归之后得到的权重也和原来的权重不同。为了检验代码的正确性，在代码末尾增加语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;predict:<span class="subst">&#123;np.dot(X_train[i], w) + b&#125;</span>, actual:<span class="subst">&#123;y_train[i]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<p>执行代码，在迭代次数较多时能明显感到执行速度变快。最终结果：<br><img src="/posts/9d18f94a/result.png" class="" title="result"><br>说明梯度下降后得到的权重能够比较好的拟合样本数据</p>
<p>损失函数值随着迭代次数增加而下降。<br><img src="/posts/9d18f94a/cost.png" class="" title="cost"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/2cfef6f1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/2cfef6f1/" class="post-title-link" itemprop="url">机器学习笔记之多元线性回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-26 14:52:30" itemprop="dateCreated datePublished" datetime="2022-09-26T14:52:30+08:00">2022-09-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、引言——多特征的房价预测"><a href="#一、引言——多特征的房价预测" class="headerlink" title="一、引言——多特征的房价预测"></a>一、引言——多特征的房价预测</h2><p>现实中，某一变量并不一定只与单一变量有关。还以房价来举例，除了位置以外，房价还可能与房子面积、卧室数量、层数、房龄等因素有关。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">面积</th>
<th style="text-align:center">卧室数</th>
<th style="text-align:center">层数</th>
<th style="text-align:center">房龄</th>
<th style="text-align:center">房价</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2104</td>
<td style="text-align:center">5</td>
<td style="text-align:center">1</td>
<td style="text-align:center">45</td>
<td style="text-align:center">460</td>
</tr>
<tr>
<td style="text-align:center">1416</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">40</td>
<td style="text-align:center">232</td>
</tr>
<tr>
<td style="text-align:center">1534</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">30</td>
<td style="text-align:center">315</td>
</tr>
<tr>
<td style="text-align:center">852</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">36</td>
<td style="text-align:center">178</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</div>
<p>对于多个特征的情况，我们需要对之前的一元线性回归进行推广。但在此之前，需要先约定一下使用的符号：</p>
<p>和前面一样，这里将用 $x^{(i)}$ 表示第 i 组样本中的特征。 $y^{(i)}$ 表示第 i 组样本对应的目标结果。不同的是，对于第 j 个<strong>特征组</strong>，也就是第j个特征对应的数值的序列，采用 $x_{j}$ 表示。那么很自然的， $x^{(i)}_j$ 表示第 j 个特征中的第 i 个值，或者说表示第 i 组样本中的第 j 个特征。</p>
<blockquote>
<p>与上文中的列表相结合进行理解，相当于 $x^{(i)}$ 表示第 i 行中非房价的部分，$x_{j}$ 则表示第 j 列中的数值。</p>
</blockquote>
<p>要对多元特征进行拟合，也就是找到适当的 $w_1, w_2, …, w_n$ 使得对任意的 i，有</p>
<script type="math/tex; mode=display">
    y^{(i)} = w_1 * x^{(i)}_1 + w_2 * x^{(i)}_2 + ... + w_n * x^{(i)}_n + b</script><p>设</p>
<script type="math/tex; mode=display">
    \vec{w} = (w_1, w_2, ... w_n)</script><script type="math/tex; mode=display">
    \vec{x}^{(i)} = (x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_n)</script><p>则原等式可以表示为</p>
<script type="math/tex; mode=display">
    y^{(i)} = \vec{w}\cdot\vec{x}^{(i)} + b</script><h2 id="二、损失函数"><a href="#二、损失函数" class="headerlink" title="二、损失函数"></a>二、损失函数</h2><p>假设给定 $\vec{w}, b$ 。对样本信息 $\vec{x}^{(i)}$ ，有对房价的预测</p>
<script type="math/tex; mode=display">
    \hat{y}^{(i)} = \vec{w}\cdot\vec{x}^{(i)} + b</script><p>与一元线性回归时同理，损失函数</p>
<script type="math/tex; mode=display">
    J(\vec{w}, b) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2 = \frac{1}{2m}\sum_{i=1}^{m}(\vec{w}\cdot\vec{x}^{(i)} + b - y^{(i)})^2</script><h2 id="三、梯度下降"><a href="#三、梯度下降" class="headerlink" title="三、梯度下降"></a>三、梯度下降</h2><p>对于损失函数，求对 $w_j$ 的偏导数：</p>
<script type="math/tex; mode=display">
    \frac{\partial{J(\vec{w}, b)}}{\partial{w_j}} = \frac{1}{m}\sum_{i=1}^{m}(\vec{w}\cdot\vec{x}^{(i)} + b - y^{(i)})x^{(i)}_j</script><p>对 $b$ 的偏导数：</p>
<script type="math/tex; mode=display">
    \frac{\partial{J(\vec{w}, b)}}{\partial{b}} = \frac{1}{m}\sum_{i=1}^{m}(\vec{w}\cdot\vec{x}^{(i)} + b - y^{(i)})</script><p>则每次迭代计算</p>
<script type="math/tex; mode=display">
    w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}</script><script type="math/tex; mode=display">
    b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}</script><p>最终就可得到拟合样本数据的多元线性方程。</p>
<h2 id="四、矩阵形式"><a href="#四、矩阵形式" class="headerlink" title="四、矩阵形式"></a>四、矩阵形式</h2><p>上述公式也可以表示为矩阵形式，设</p>
<script type="math/tex; mode=display">
    Y = \begin{bmatrix}
        y^{(1)}&y^{(2)}&\cdots&y^{(m)}
    \end{bmatrix}^T</script><script type="math/tex; mode=display">
    W = \begin{bmatrix}
        w_1&w_2&\cdots&w_n
    \end{bmatrix}^T</script><script type="math/tex; mode=display">
    X = \begin{bmatrix}
        x^{(1)}_1 & x^{(1)}_2 & \cdots & x^{(1)}_n \\
        x^{(2)}_1 & x^{(2)}_2 & \cdots & x^{(2)}_n \\
        \vdots&\vdots& \ddots & \vdots \\
        x^{(m)}_1 & x^{(m)}_2 & \cdots & x^{(m)}_n
    \end{bmatrix}</script><script type="math/tex; mode=display">
    B = \begin{bmatrix}
        b&b&\cdots&b
    \end{bmatrix}_{1\times{m}}^T</script><p>则原等式可以表示为</p>
<script type="math/tex; mode=display">
    Y = XW + B</script><p>对特定的 $W, B$ 对当前样本 $X$ 的预测为</p>
<script type="math/tex; mode=display">
    \hat{Y} = XW + B</script><p>则损失函数可以表示为</p>
<script type="math/tex; mode=display">
    J(W, B) = \frac{1}{2m}(Y - \hat{Y})^T(Y - \hat{Y})</script><p>设</p>
<script type="math/tex; mode=display">
    G = \begin{bmatrix}
        \frac{\partial{J(W, B)}}{w_1}&\frac{\partial{J(W, B)}}{w_2}&\cdots&\frac{\partial{J(W, B)}}{w_n}
    \end{bmatrix}^T</script><p>则有</p>
<script type="math/tex; mode=display">
    G = \frac{1}{m}X^T(\hat{Y} - Y)</script><p>另有</p>
<script type="math/tex; mode=display">
    \frac{\partial{J(W, B)}}{b} = \frac{1}{m}sum(\hat{Y} - Y)</script><p>其中 $sum$ 表示对向量上各元素求和</p>
<blockquote>
<p>注意公式只经过个人推导，不保证正确性</p>
</blockquote>
<h2 id="五、代码实现"><a href="#五、代码实现" class="headerlink" title="五、代码实现"></a>五、代码实现</h2><h3 id="常规形式"><a href="#常规形式" class="headerlink" title="常规形式"></a>常规形式</h3><p>同样的，我们先导入<code>numpy</code>和<code>Matplotlib</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure></p>
<p>之后定义一些需要用到的函数。</p>
<p>计算损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_cost</span>(<span class="params">w, b, X, y</span>):</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    sum_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        actual_y = np.dot(X[i], w) + b</span><br><span class="line">        sum_cost += (actual_y - y[i]) ** <span class="number">2</span></span><br><span class="line">    total_cost = sum_cost / (<span class="number">2</span> * m)</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure></p>
<p>计算损失函数对应的梯度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_gradient</span>(<span class="params">w, b, X, y</span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line"></span><br><span class="line">    sum_w = np.zeros((n, ))</span><br><span class="line">    sum_b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        err = (np.dot(w, X[i]) + b - y[i])</span><br><span class="line">        sum_b += err</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sum_w[j] += X[i,j] * err</span><br><span class="line"></span><br><span class="line">    gradients_w = sum_w / m</span><br><span class="line">    gradient_b = sum_b / m</span><br><span class="line">    <span class="keyword">return</span> gradients_w, gradient_b</span><br></pre></td></tr></table></figure></p>
<p>进行一次梯度下降迭代<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">w, b, X, y, alpha</span>):</span><br><span class="line">    d_w, d_b = get_gradient(w, b, X, y)</span><br><span class="line">    w = w - alpha * d_w</span><br><span class="line">    b = b - alpha * d_b</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure></p>
<p>接下来是主体部分</p>
<p>设置已知数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.array(</span><br><span class="line">    [[<span class="number">2104</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">45</span>],</span><br><span class="line">    [<span class="number">1416</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">40</span>],</span><br><span class="line">    [<span class="number">852</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">35</span>]])</span><br><span class="line">y_train = np.array([<span class="number">460</span>, <span class="number">232</span>, <span class="number">178</span>])</span><br></pre></td></tr></table></figure><br>初始化 $w$, $b$ 及步长 $\alpha$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = np.zeros((X_train.shape[<span class="number">1</span>], ))</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">alpha = <span class="number">1e-7</span></span><br></pre></td></tr></table></figure></p>
<p>使用梯度下降迭代 1000 次。每 10 次打印当前的信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    w, b = gradient_descent(w, b, X_train, y_train, alpha)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;times:<span class="subst">&#123;i&#125;</span>, w:<span class="subst">&#123;np.around(w, <span class="number">3</span>)&#125;</span>, b:<span class="subst">&#123;b:<span class="number">0.2</span>f&#125;</span>, cost:<span class="subst">&#123;get_cost(w, b, X_train, y_train):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><br><img src="/posts/2cfef6f1/print.png" class="" title="print"></p>
<p>代价随着迭代次数的增加而减少<br><img src="/posts/2cfef6f1/loss.png" class="" title="loss"></p>
<h3 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h3><p>可以利用矩阵形式重写损失函数计算和梯度计算。<br>重写损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_cost</span>(<span class="params">w, b, X, y</span>):</span><br><span class="line">    m= X.shape[<span class="number">0</span>]</span><br><span class="line">    B = np.ones((m, )) * b</span><br><span class="line">    actual_y = np.matmul(X, w.T) + B</span><br><span class="line">    err = actual_y - y.T</span><br><span class="line">    total_cost = np.matmul(err.T, err) / (<span class="number">2</span> * m)</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure><br>重写梯度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_gradient</span>(<span class="params">w, b, X, y</span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line">    </span><br><span class="line">    sum_w = np.zeros((n, ))</span><br><span class="line">    sum_b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    B = np.ones((m, )) * b</span><br><span class="line">    y_hat = np.matmul(X, w.T) + B</span><br><span class="line">    sum_w = np.matmul(X.T, y_hat - y.T)</span><br><span class="line">    sum_b = np.<span class="built_in">sum</span>(y_hat - y.T)</span><br><span class="line"></span><br><span class="line">    gradients_w = sum_w / m</span><br><span class="line">    gradient_b = sum_b / m</span><br><span class="line">    <span class="keyword">return</span> gradients_w, gradient_b</span><br></pre></td></tr></table></figure><br>运行，与重写之前结果相同<br><img src="/posts/2cfef6f1/printMatrix.png" class="" title="printMatrix"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wokron.github.io/posts/5371358e/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="wokron">
      <meta itemprop="description" content="StringCat的个人博客。记录学习、分享经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="StringCat的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/5371358e/" class="post-title-link" itemprop="url">机器学习笔记之一元线性回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-25 10:22:13" itemprop="dateCreated datePublished" datetime="2022-09-25T10:22:13+08:00">2022-09-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-26 10:38:38" itemprop="dateModified" datetime="2023-06-26T10:38:38+08:00">2023-06-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、引言——房价预测问题"><a href="#一、引言——房价预测问题" class="headerlink" title="一、引言——房价预测问题"></a>一、引言——房价预测问题</h2><p><strong>问题：</strong><br>假设我们知道一些房价与距离的对应关系，通过这些已知的信息，能否预测在一定范围内任意距离对应的房价？</p>
<p>首先考虑最为简单的情况，也就是只有两个房价信息的情况。<br><img src="/posts/5371358e/simpleScatterPrice.png" class="" title="scatter"></p>
<p>如图所示，1km时对应房价为300，2km 时对应房价为500。很容易可以在这两点间连一条直线，这条直线就可以作为对房价的预测。这样用曲线对已知数据中的关系进行估计的方法称为拟合。</p>
<blockquote>
<p>（注意，本文只考虑以一元线性函数进行拟合）</p>
</blockquote>
<img src="/posts/5371358e/simplePredict.png" class="" title="predict">
<p>类似的，增加房价信息为三个点，如果三点共线，则该直线依然可以作为对房价的拟合。<br><img src="/posts/5371358e/threePredict.png" class="" title="predict2"></p>
<p>但更可能的情况是三点不共线，对于更多的数据的情况则更是如此。这样的话要如何找到一条直线来拟合房价信息呢？</p>
<h2 id="二、损失函数"><a href="#二、损失函数" class="headerlink" title="二、损失函数"></a>二、损失函数</h2><p>我们需要找到一种标准来衡量直线对已有信息的拟合程度。</p>
<p>假设当前直线为：</p>
<script type="math/tex; mode=display">
    y = wx+b</script><p>房子距离为 $x^{(1)},x^{(2)},x^{(3)}…,x^{(m)}$，对应的房价为 $y^{(1)},y^{(2)},y^{(3)}…,y^{(m)}$。<br>则对于每个距离 $x^{(i)}$，当前直线所预测的房价为:</p>
<script type="math/tex; mode=display">
    \hat{y}^{(i)} = wx^{(i)} + b</script><p>该房价与真正的房价的差距为</p>
<script type="math/tex; mode=display">
    d = |y^{(i)} - \hat{y}^{(i)}|</script><p>绝对值不可导，不妨用平方来代替</p>
<script type="math/tex; mode=display">
    d_2 = (y^{(i)} - \hat{y}^{(i)})^2</script><p>对于每个已知的房价信息，当前直线的预测都可能会有偏差，将这些偏差求和得到总的偏差</p>
<script type="math/tex; mode=display">
    \sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2</script><p>为了排除样本数量不同对偏差的影响，将偏差总和除以样本数量</p>
<script type="math/tex; mode=display">
    \frac{1}{m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2</script><p>这也就是总体方差的计算式：</p>
<script type="math/tex; mode=display">
    \sigma^2 = \frac{1}{m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2</script><p>对于机器学习，我们将$\hat{y}^{(i)}$展开，并将方差除以2，得到一元线性回归的损失函数（loss function），即</p>
<script type="math/tex; mode=display">
    J(w, b) = \frac{1}{2m}\sum_{i=1}^{m}(y^{(i)} - wx^{(i)} - b)^2</script><p>在 $x^{(i)}$，$y^{(i)}$ 确定的情况下，损失函数是关于 $w$ 和 $b$ 的连续可导的二元函数。该函数可以用来衡量 $w，b$ 所确定的直线与已知数据之间的偏差。</p>
<h2 id="三、梯度下降"><a href="#三、梯度下降" class="headerlink" title="三、梯度下降"></a>三、梯度下降</h2><p>既然我们已经可以用 $J(w, b)$ 衡量直线对数据的拟合程度了，那么对于“如何找到最佳的拟合曲线”这一问题，就转化为了“如何找到使得 $J(w, b)$ 最小的 $w，b$ 值”。</p>
<p>我们可以确定一个任意的点 $(w_0, b_0)$ 作为初始位置，那么要到达 $J(w, b)$ 最小的位置， 要如何移动呢？我们要向能使 $J(w, b)$ 减小，并减小得最快的方向移动。具体的，也就是向着梯度的反方向移动。</p>
<script type="math/tex; mode=display">
    (w_1, b_1) = (w_0, b_0) - \alpha\nabla{J(w, b)}</script><p>或</p>
<script type="math/tex; mode=display">
    \left\{
        \begin{array}{lr}
            w_1 = w_0 - \alpha\frac{\partial{J(w, b)}}{\partial{w}} & \\
            & \\
            b_1 = b_0 - \alpha\frac{\partial{J(w, b)}}{\partial{b}}
        \end{array}
    \right.</script><p>其中 $\alpha$ 称为步长，表示一次迭代时的移动程度。</p>
<p>通过多次重复迭代这一过程，最终得到的 $(w_n, b_n)$ 将位于梯度为 0 的位置。即损失函数的一个极小值点。这时对应的直线将能较好地拟合原本的数据。</p>
<blockquote>
<p>当然，极小值点不意味着对应的值便是损失函数的最小值。在一些情况下，这一算法也可能会使拟合的直线处于局部最优而非全局最优的情况。但本文暂不处理这种情况。</p>
</blockquote>
<h2 id="四、代码实现"><a href="#四、代码实现" class="headerlink" title="四、代码实现"></a>四、代码实现</h2><p>我们先导入<code>numpy</code>和<code>Matplotlib</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure></p>
<p>之后定义一些需要用到的函数。</p>
<p>计算损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_cost</span>(<span class="params">w, b, x, y</span>):</span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    sum_cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        actual_y = w * x[i] + b</span><br><span class="line">        sum_cost += (actual_y - y[i]) ** <span class="number">2</span></span><br><span class="line">    total_cost = (<span class="number">1</span> / (<span class="number">2</span> * m)) * sum_cost</span><br><span class="line">    <span class="keyword">return</span> total_cost</span><br></pre></td></tr></table></figure></p>
<p>计算损失函数对应的梯度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_gradient</span>(<span class="params">w, b, x, y</span>):</span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    sum_w = <span class="number">0</span></span><br><span class="line">    sum_b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        sum_w += x[i] * (w * x[i] + b - y[i])</span><br><span class="line">        sum_b += (w * x[i] + b - y[i])</span><br><span class="line">    gradient_w = (<span class="number">1</span> / m) * sum_w</span><br><span class="line">    gradient_b = (<span class="number">1</span> / m) * sum_b</span><br><span class="line">    <span class="keyword">return</span> gradient_w, gradient_b</span><br></pre></td></tr></table></figure></p>
<p>进行一次梯度下降迭代<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">w, b, x, y, alpha</span>):</span><br><span class="line">    d_w, d_b = get_gradient(w, b, x, y)</span><br><span class="line">    w -= alpha * d_w</span><br><span class="line">    b -= alpha * d_b</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure></p>
<p>接下来是主体部分</p>
<p>设置已知数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = np.array([<span class="number">1.0</span>, <span class="number">1.7</span>, <span class="number">2.0</span>, <span class="number">2.5</span>, <span class="number">3.0</span>, <span class="number">3.2</span>])</span><br><span class="line">y_train = np.array([<span class="number">250</span>, <span class="number">300</span>, <span class="number">480</span>, <span class="number">430</span>, <span class="number">630</span>, <span class="number">730</span>,])</span><br></pre></td></tr></table></figure><br>初始化 $w$, $b$ 及步长 $\alpha$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = <span class="number">0</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">alpha = <span class="number">1e-2</span></span><br></pre></td></tr></table></figure></p>
<p>使用梯度下降迭代 20000 次。每 1000 次打印当前的信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    w, b = gradient_descent(w, b, x_train, y_train, alpha)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;w:<span class="subst">&#123;w&#125;</span>, b:<span class="subst">&#123;b&#125;</span>, cost:<span class="subst">&#123;get_cost(w, b, x_train, y_train)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><br><img src="/posts/5371358e/print.png" class="" title="print"></p>
<p>最后输出拟合结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.xlabel(<span class="string">&quot;house location(km)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;price($)&quot;</span>)</span><br><span class="line">plt.scatter(x_train, y_train)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">4</span>], [b, w*<span class="number">4</span>+b])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>拟合效果如图：<br><img src="/posts/5371358e/actualPredict.png" class="" title="predict3"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wokron"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">wokron</p>
  <div class="site-description" itemprop="description">StringCat的个人博客。记录学习、分享经验</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wokron" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wokron" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/stringcatwok@gmail.com" title="E-Mail → stringcatwok@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wokron</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      总访问人数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      总阅读次数：<span id="busuanzi_value_site_pv"></span>
    </span>
</div>







      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
