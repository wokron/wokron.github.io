---
title: 机器学习之神经网络基础
tags:
  - AI
  - 机器学习
categories: 学习笔记
mathjax: true
abbrlink: 76c44d63
date: 2022-10-14 19:25:16
---

## 一、神经元模型
在研究人工智能的过程中，模拟生物的神经是一条很显然的道路。神经网络模型最初便是以模拟生物的神经网络为目的的。但是在早期算力不足的情况下，神经网络的效果并不怎么好。直到硬件基础成熟的最近一段时间，神经网络才发挥出超常的能力。

当然，此时经过改进、优化后的神经网络也在一定程度上偏离了最初模拟生物神经系统的初衷。当前的，以统计为基础的神经网络模型是否是实现强人工智能的有效方法也不得而知。但是我们还是有必要对神经网络模型，特别是作为神经网络的最小单元——神经元模型，做充分的介绍。因为事实证明了，这是一条有效的创造 AI 工具的途径。

单个神经元可以看做有多个输入 $x_j$，单个输出 $a$ 的函数。神经元模型具有如下属性：
- 权重 $\vec{w}, b$
- 激发函数 $g(z)$

所对应的函数为
$$
    f_{\vec{w}, b, g(z)}(\vec{x}) = g(\vec{w} \cdot \vec{x} + b)
$$

> 取 $g(z) = \frac{1}{1 + e^{-z}}$, 与我们之前学习过的逻辑回归没有差别。

通过对这样的神经元按一定规则进行组合，便可以构造用于完成某种功能的神经网络。

## 二、神经网络层
对于一组输入，我们可以将其依次通过不同的神经元进行计算，计算的结果可以按顺序组成向量，作为一组新的输入。这种不断迭代的能力是神经网络的基础。我们称同一次计算使用的所有神经元为**同一层**的神经元。这些神经元组成了神经网络上的一**层**。不同层的组合构成了整个神经网络。

> 注意，同一层的神经元需要满足输入个数相同、激发函数相同。

从数学上讲，设输入为 $\vec{x} = (x_1, x_2, ..., x_n)$，输出为 $\vec{a} = (a_1, a_2, ..., a_m)$，神经元函数为 $f_1(\vec{x}), f_2(\vec{x}), ... f_m(\vec{x})$ 则
$$
    \vec{a} = (f_1(\vec{x}), f_2(\vec{x}), ... f_m(\vec{x}))
$$

对于不同层上的属性或参数 x，我们加上上标 $x^{[i]}$ 表示该属性或参数属于第 i 层（第几层指示神经元计算的顺序）；对于同一层的不同神经元，加上下标 $x^{[i]}_j$ 表示第 i 层的第 j 个神经元的属性或参数。

则更一般的，假设第 i 层接受 n 个参数的输入，并具有 m 个神经元，则第 i 层上的运算可表示为：
$$
        \left\{
        \begin{array}{lr}
            a^{[i]}_1 = g(\vec{w}^{[i]}_1 \cdot \vec{a}^{[i-1]} + b^{[i]}_1) \\
            a^{[i]}_2 = g(\vec{w}^{[i]}_2 \cdot \vec{a}^{[i-1]} + b^{[i]}_2) \\
            \vdots \\
            a^{[i]}_m = g(\vec{w}^{[i]}_m \cdot \vec{a}^{[i-1]} + b^{[i]}_m)
        \end{array}
    \right.
$$
> $\vec{a}^{[0]}$ 表示神经网络的总输入

## 三、神经网络的矩阵表示
神经网络所以一种有效的机器学习方法，在于其清晰的结构和高效的计算方式。清晰的结构已经在上一部分有所说明，这里进一步阐述神经网络的矩阵表示形式。通过矩阵并行化计算，神经网络的运行速度将得到极大地提升。

对于单一神经元上的权重，我们可以将其表示成列向量的形式，即
$$
    W^{[i]}_j = \begin{bmatrix}
        w_1 & w_2 & \cdots & w_n
    \end{bmatrix}^T
$$

按照上一节中的公式，要进行点乘运算，则应该将输入表示为行向量的形式
$$
    A^{[i-1]} = \begin{bmatrix}
        a_1 & a_2 & \cdots & a_n
    \end{bmatrix}
$$

则 $\vec{w}^{[i]}_j \cdot \vec{a}^{[i-1]}$ 就可以转化为 $A^{[i-1]}W^{[i]}_j$。

之后，我们可以先忽略激发函数。只看激发函数内的一组线性表达式，将这组表达式的结果也按顺序表示为行向量
$$
    Z^{[i]} = \begin{bmatrix}
        z^{[i]}_1 & z^{[i]}_2 & \cdots & z^{[i]}_m
    \end{bmatrix}
$$

把 b 的部分提出单独作为一个向量，记
$$
    B^{[i]} = \begin{bmatrix}
        b_1 & b_2 & \cdots & b_m
    \end{bmatrix}
$$
则
$$
    Z^{[i]} = \begin{bmatrix}
        A^{[i-1]}W^{[i]}_1 & A^{[i-1]}W^{[i]}_2 & \cdots & A^{[i-1]}W^{[i]}_m
    \end{bmatrix} + B^{[i]}
$$

再将输入 $A^{[i-1]}$ 提出来
$$
    Z^{[i]} = A^{[i-1]} \begin{bmatrix}
        W^{[i]}_1 & W^{[i]}_2 & \cdots & W^{[i]}_m
    \end{bmatrix} + B^{[i]}
$$

此时各个神经元的权重组合成了一个新的矩阵，记
$$
    W^{[i]} = \begin{bmatrix}
        W^{[i]}_1 & W^{[i]}_2 & \cdots & W^{[i]}_m
    \end{bmatrix}_{n \times m}
$$

> 可以认为，神经网络的层的本质就是矩阵 $W^{[i]}$ 和向量 $B^{[i]}$

此时如果我们认为函数 $g(Z)$ 表示对矩阵中的每一个元素应用激活函数 $g(z)$ ，则整个神经网络层就可以表示为
$$
    A^{[i]} = g(A^{[i-1]} W^{[i]} + B^{[i]})
$$

这不只是形式上的化简。利用 numpy 等数学库，可以极大加快运算的速度。